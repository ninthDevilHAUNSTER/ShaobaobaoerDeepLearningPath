# 若干种梯度更新方法

## SGD

之前用得最多的，随机梯度下降法

#### 优点

简单易懂

#### 缺点

对于非均相的图形路径不敏感，需要很久才能收敛

#### 公式

$$W \longleftarrow W - \eta \cfrac{\partial L}{\partial W}$$

```python
def update(self, params, grads):
    for key in params.keys():
        params[key] -= self.lr * grads[key]
```

### Momentum

动量，其值就好像在斜面上的小球。类似于最小二乘回归的曲线样子

#### 公式

$$v \longleftarrow \alpha v - \eta \cfrac{\partial L}{\partial W}$$

$$W \longleftarrow W + v$$

#### 代码

```python
def update(self, params, grads):
    if self.v is None:
        self.v = {}
        for key , val in params.items():
            self.v[key] = np.zeros_like(val)
            
    for key in params.keys():
        self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
        params[key] += self.v[key]
```

### AdaGrad

记录所有梯度的平方和，引入一种学习率衰减的概念。

#### 公式

$$h \longleftarrow h + \cfrac{\part L }{\part W } ·   \cfrac{\part L }{\part W } $$

$$W \longleftarrow - \cfrac {\eta}{\sqrt{h}} \cfrac{\part L}{\part W}$$

- 那个· 是矩阵乘法，损失函数关于W的梯度
- h 为之前所有梯度值的平方和
- 相当于在W运算的时候乘以了一个参数，来调整学习尺度
  - 对于更新较大的参数，学习率将会变小，就是如此

#### 代码

```python
def update(self, params, grads):
    if self.h is None:
        self.h = {}
        for key , val in params.items():
            self.h[key] = np.zeros_like(val)
            
    for key in params.keys():
        self.h[key] += grads[key] * grads[key]
        params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)  
```

### Adam 

这个比较难，先不说了





